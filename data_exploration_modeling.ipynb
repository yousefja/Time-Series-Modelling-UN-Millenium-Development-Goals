{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                 United Nations Millenium Development Goals\n",
    "                                            DriveData Competition\n",
    "\n",
    "                        Data Exploration/Viz, Model Construction, and Goal Prediction\n",
    "                                              by Yuseof Jaber\n",
    "                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In this project I refer to 'Features' and 'Indicators' interchangeably, as well as 'Goals' and 'Targets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "# Material that aided me in this project:\n",
    "\n",
    "    1. Sequences, Time Series, and Predictions (Coursera and DeepLearning.ai)\n",
    "        https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction\n",
    "       \n",
    "    2. Iterative Imputation\n",
    "        https://machinelearningmastery.com/iterative-imputation-for-missing-values-in-machine-learning/\n",
    "    \n",
    "    3. Correlation Tests and Feature Selection\n",
    "        https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b\n",
    "        https://www.datacamp.com/community/tutorials/feature-selection-python\n",
    "        \n",
    "    4. Standardization vs Normalization\n",
    "    https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff\n",
    "    \n",
    "    5. CNNs for Time Series Forecasting\n",
    "    https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data manipulation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#graphing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import prettyplotlib as pplt\n",
    "\n",
    "#imputing\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "#data normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Keras\n",
    "import keras\n",
    "from keras.layers import Input, Dense, LSTM, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "#Errors\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read Data from CSV files\n",
    "data = pd.read_csv('./data/TrainingSet.csv', index_col = 0)\n",
    "submission = pd.read_csv('./SubmissionRows.csv', index_col = 0)\n",
    "data_original = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data exploration\n",
    "\n",
    "#214 countries in the dataset\n",
    "countries = data['Country Name'].unique()\n",
    "\n",
    "#print(countries)\n",
    "\n",
    "#get the MDGs from the submission file\n",
    "mdgs = submission.index\n",
    "\n",
    "#Exploring Targets for prediction\n",
    "targets_index = data.loc[mdgs].index\n",
    "prediction_rows = data.loc[mdgs]\n",
    "\n",
    "#Amount of rows (indicators?) per country\n",
    "rows_country = data.groupby('Country Name')['Series Name'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of the goals for all the countries in the dataset\n",
    "goals_by_country = prediction_rows.groupby('Country Name')[['Country Name','Series Name']]\n",
    "\n",
    "def get_targets_and_indicators_by_country(country, dataset, mdgs):\n",
    "    \n",
    "    '''\n",
    "    Aptly named function \n",
    "    \n",
    "    args:\n",
    "        country -- the country being queried\n",
    "        dataset -- which dataset to pull indicators from\n",
    "        mdgs    -- indices (from submission csv) \n",
    "    \n",
    "    returns:\n",
    "        country_indicators -- DataFrame containing all rows from dataset for country specified\n",
    "        country_targets    -- DataFrame containing all rows that represent the targets to be predicted for country specified\n",
    "    '''\n",
    "    \n",
    "    if (country not in data['Country Name'].values):\n",
    "        print(\"*WARNING* Can't find %s in dataset, returning null values\" % country)\n",
    "        return None, None\n",
    "        \n",
    "    country_indicators = dataset[dataset['Country Name'] == country]\n",
    "    country_targets = country_indicators.loc[set(mdgs.values) & set(country_indicators.index.values)]\n",
    "    \n",
    "    return  country_targets, country_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "---------------------------  PROGRESS TOWARDS MDGs ---------------------------------------\n",
    "\n",
    "'''\n",
    "\n",
    "targets, indicators = get_targets_and_indicators_by_country(\"Israel\", data, mdgs)\n",
    "country = targets['Country Name'].iloc[0]\n",
    "\n",
    "years = [x for x in range(1972, 2008, 1)]\n",
    "\n",
    "#Make series name the index for plot legend\n",
    "targets = targets.set_index('Series Name')\n",
    "\n",
    "#remove non numeric info\n",
    "targets = targets.select_dtypes(exclude = ['object'])\n",
    "\n",
    "#'1972 yr[1972]' ---> 1972\n",
    "targets.columns = years\n",
    "\n",
    "#flip so year is x-axis\n",
    "targets = targets.transpose()\n",
    "\n",
    "#get rid of nulls\n",
    "targets = targets.mask(targets.isnull())\n",
    "\n",
    "#plot config\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('MDG Progress for %s' % country)\n",
    "\n",
    "\n",
    "line = sns.lineplot(data = targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statistics \n",
    "#Get overlapping Indicators between countries\n",
    "target_Z, indic_Z = get_targets_and_indicators_by_country(\"American Samoa\", data, mdgs)\n",
    "A_list = indicators['Series Name'].values.tolist()\n",
    "Z_list = indic_Z['Series Name'].values.tolist()\n",
    "\n",
    "overlap = set(A_list) & set(Z_list)\n",
    "\n",
    "\n",
    "'''\n",
    "--------------------  INFORMATION ABOUT INDICATORS --------------------\n",
    "\n",
    "I will be using the value 934 as a baseline number of indicators (and \n",
    "therefore features for the models). \n",
    "\n",
    "'''\n",
    "\n",
    "max = data.groupby('Country Name')['Series Name'].count().max()    #1255\n",
    "min = data.groupby('Country Name')['Series Name'].count().min()    #13\n",
    "avg = data.groupby('Country Name')['Series Name'].count().mean()   #913\n",
    "mode = data.groupby('Country Name')['Series Name'].count().mode()  #887, 934, 1196, 1212\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all possible targets to be predicted\n",
    "#x = data.loc[mdgs].groupby('Country Name')['Series Name'].count() == 6\n",
    "target_data = data.loc[mdgs]\n",
    "target_names = target_data['Series Name'].unique()\n",
    "target_names.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                                INDICATOR --> GOAL GENERALIZED MODEL\n",
    "                                        \n",
    "            x_input  -->  vec_indicators =  [ i1, i2 ... i(no_indicators_for_model)]\n",
    "            y_output -->  vec_goals =       [ goal 1. . . goal 7]    \n",
    "\n",
    "    This disregards the country from where the indicators are taken and seeks to find a more general relationship between the indicators and the goals for a specific year. Years are also ditched (after creating the DataFrames so that indicators map to the goal value for that year)\n",
    "    \n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "def convert_to_datetime(str):\n",
    "   \n",
    "    '''\n",
    "    This function will be used in order to map the years in format '1972 [YR1972]' to a datetime object \n",
    "    '1972 - 01 - 01'\n",
    "    '''\n",
    "    \n",
    "    str = str.split(' ')[0]\n",
    "    str = datetime.datetime.strptime(str, '%Y')\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Data Wrangling\n",
    "\n",
    "SORTING DATA INTO SEPERATE INDICATOR AND GOAL DATAFRAMES\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators_for_model(indicator_threshold, indicator_dataframe):\n",
    "    \n",
    "    '''\n",
    "    Sorts indicators by the amount of countries that have them, then picks all of the indicators until \n",
    "    it reaches one with indicator_threshold number of countries \n",
    "    \n",
    "    args:\n",
    "        indicator_threshold -- the minimum amount of countries with a given indicator\n",
    "        indicator_dataframe -- dataframe to pull indicators from\n",
    "        \n",
    "    returns:\n",
    "        most_common_indicators -- DataFrame with all indicators with count above indicator threshold\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #get(no_indicators_for_model) most common indicators in alphabetical order\n",
    "    indicator_count = indicator_dataframe.groupby('Series Name')['Country Name'].count()\n",
    "    indicator_count.sort_values(ascending = False)\n",
    "\n",
    "    most_common_indicators = indicator_count[indicator_count >= indicator_threshold]\n",
    "    most_common_indicators_list = most_common_indicators.index.sort_values().tolist()\n",
    "    \n",
    "    return most_common_indicators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_indicators(feature_dataframe, indicators_list):\n",
    "    \n",
    "    '''\n",
    "    Takes the feature dataframe, checks which indicators from the desired_indicators_list it doesn't have, \n",
    "    and adds Nan filled rows for each missing indicator\n",
    "\n",
    "    args:\n",
    "        feature_dataframe -- targets removed, indexed by [Country Name, Series Name]\n",
    "        indicators_list -- returned from get_indicators_for_model\n",
    "\n",
    "    '''\n",
    "\n",
    "    #used for column names to populate dataframe for missing indicators\n",
    "    years = feature_dataframe.columns\n",
    "\n",
    "    #now add missing indicators and set values to nan for each country (NOTE! this can probably be done more efficiently)\n",
    "    for country in countries:\n",
    "\n",
    "        #get indicators for current country (as a DataFrame)\n",
    "        current = feature_dataframe.loc[country]\n",
    "\n",
    "        #determine which indicators are missing from current\n",
    "        missing_indicators_list = set(indicators_list) - set(current.index.tolist())\n",
    "        missing_count = len(missing_indicators_list)\n",
    "\n",
    "        #create indices to match the multiindicies of the X_features multiindex df\n",
    "        index_arrays = [np.array([str(country) for x in range(missing_count)]), np.array(list(missing_indicators_list))]\n",
    "\n",
    "        #create a matrix of Nan values to be used for the missing indicator values\n",
    "        nan_matrix = np.empty((missing_count, len(years)))\n",
    "        nan_matrix[:] = np.nan\n",
    "\n",
    "        #create a multiindex df for the missing indicators to append to the X_features df\n",
    "        missing_indicators_midf = pd.DataFrame(nan_matrix, index = index_arrays, columns = years)\n",
    "        feature_dataframe = feature_dataframe.append(missing_indicators_midf)\n",
    "\n",
    "    #appending adds rows to end of df rather than end of rows with same index. Sorting fixes this\n",
    "    feature_dataframe = feature_dataframe.sort_index()\n",
    "\n",
    "    #Check to see that all countries have the same number of indicators\n",
    "    checker = feature_dataframe.reset_index().groupby('Country Name')['Series Name'].count()\n",
    "    assert(checker.iloc[0] > 0)\n",
    "    assert(checker.sum() == checker.iloc[0] * len(checker))\n",
    "    \n",
    "    #reformat years so they are in datetime format\n",
    "    reformatted_cols = feature_dataframe.columns.map(convert_to_datetime).tolist()\n",
    "    feature_dataframe.columns = reformatted_cols\n",
    "\n",
    "    #essentially swapping the years columns with the 'Series Name' index. In a sense, transposing\n",
    "    feature_dataframe_stacked = feature_dataframe.stack(dropna = False)\n",
    "    feature_dataframe = feature_dataframe_stacked.unstack(1)\n",
    "    \n",
    "    return feature_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# ALTERNATE IMPUTATION METHODS FOR STATISTICAL MODELS\n",
    "----\n",
    "\n",
    "These were developed in the early stages of the project. Might Still be useful, just not for current models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#TODO: Fix logic for nan checking\n",
    "\n",
    "def split_and_impute(features, targets, x_strategy, y_strategy, test_size):\n",
    "    '''\n",
    "    Vary impute methods and test_size to allow for testing models with multiple train and test sets\n",
    "    \n",
    "    args:\n",
    "        features -- indicator dataframe (no index)\n",
    "        targets  -- goal dataframe (no index)\n",
    "        x_strategy -- how to impute indicators\n",
    "        y_strategy -- how to impute goals \n",
    "        test_size -- for splitting data\n",
    "    \n",
    "    returns:\n",
    "        X_train_imputed\n",
    "        X_test_imputed\n",
    "        y_train_imputed\n",
    "        y_test_imputed\n",
    "    '''\n",
    "    \n",
    "    t_check = True\n",
    "    t1, t2, t3, t4 =  None, None, None, None \n",
    "    \n",
    "    while(t_check):\n",
    "        #split into sets, make sure the test split doesn't create any all-nan columns in subsets (making imputing impossible)\n",
    "        resplit_not_successful = True\n",
    "        X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "        while(resplit_not_successful == True):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size = test_size, shuffle = True)\n",
    "            sets_for_na_check = [X_train, X_test, y_train, y_test]   \n",
    "\n",
    "            #false if all nan, meaning resplit is desired\n",
    "            for i in sets_for_na_check:\n",
    "                resplit_not_successful = resplit_not_successful and i.notna().any().any()\n",
    "\n",
    "            #exit when resplit_not_successful is false \n",
    "            resplit_not_successful = resplit_not_successful != True\n",
    "\n",
    "        #Add on to functionality later, for now x will always be simple imputer and y fillna()\n",
    "\n",
    "        #Features\n",
    "        if(x_strategy == 'simple_mean'):\n",
    "            imputer_x = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "        elif(x_strategy == 'simple_median'):\n",
    "            imputer_x = SimpleImputer(strategy = 'median')\n",
    "\n",
    "        elif(x_strategy == 'simple_frequent'):\n",
    "            imputer_x = SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "        X_train_imputed = pd.DataFrame(imputer_x.fit_transform(X_train), columns = X_train.columns)\n",
    "        X_test_imputed  = pd.DataFrame(imputer_x.transform(X_test), columns = X_test.columns)\n",
    "\n",
    "        #Targets\n",
    "        if(y_strategy == 'fill_mean'):\n",
    "            y_train_imputed = y_train.fillna(y_train.mean())\n",
    "            y_test_imputed = y_test.fillna(y_test.mean())\n",
    "\n",
    "        elif(y_strategy == 'fill_median'):\n",
    "            y_train_imputed = y_train.fillna(y_train.median())\n",
    "            y_test_imputed = y_test.fillna(y_test.median())\n",
    "\n",
    "        elif(y_strategy == 'fill_mode'):\n",
    "            y_train_imputed = y_train.fillna(y_train.mode())\n",
    "            y_test_imputed = y_test.fillna(y_test.mode())\n",
    "\n",
    "\n",
    "        #make sure no nan, if so call function again\n",
    "        t1, t2, t3, t4 =  pd.isnull(X_train_imputed).any().any(), pd.isnull(X_test_imputed).any().any(), pd.isnull(y_train_imputed).any().any(), pd.isnull(y_test_imputed).any().any()\n",
    "        t_check = t1 or t2 or t3 or t4\n",
    "            \n",
    "    print('t_check passed if false: ', t_check)\n",
    "    return X_train_imputed, X_test_imputed, y_train_imputed, y_test_imputed, y_test, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_interpolate(filled_indicators_df, n_rows, n_knn_neighbors, d, knn_weights = None):\n",
    "    '''\n",
    "    Use KNN imputation to fill in a subset of the rows for each country. Then use interpolation to fill in\n",
    "    the rest of the rows (so that they make sense as a progression over time)\n",
    "    \n",
    "    args:\n",
    "        filled_indicators_df -- unimputed, indicator dataframe with missing indicators filled in\n",
    "        n_rows -- how many vectors/rows for each country to use for KNN imp\n",
    "        n_knn_neighbors -- how many neighbors to consider for knn imp\n",
    "        knn_weights -- knn imputer paramter. 'uniform' by default, otherwise 'distance'\n",
    "        \n",
    "    returns:\n",
    "        X_train_imputed\n",
    "        X_test_imputed\n",
    "        y_train_imputed\n",
    "        y_test_imputed\n",
    "    '''\n",
    "\n",
    "    #check if passed dataframe has any columns with all nan values\n",
    "    if(X_features_filled.isnull().all().any() == True):\n",
    "        print(\"The dataframe contains columns with all nan values\")\n",
    "    \n",
    "    \n",
    "    #default knn_weights parameter\n",
    "    if(knn_weights in (['uniform', 'distance']) == False):\n",
    "        knn_weights == 'uniform'\n",
    "        \n",
    "    #1. Pick vectors for KNN imputation\n",
    "    n_rows -= 4 #since I'm already adding the first and last years\n",
    "\n",
    "    checker = True\n",
    "    \n",
    "    while(checker == True):\n",
    "        \n",
    "        n_rows += 3\n",
    "            \n",
    "        print(\"increased rows to: \", n_rows)\n",
    "        \n",
    "        grabbed = [years[i] for i in [0, -1]]\n",
    "\n",
    "        for i in range(1, n_rows):\n",
    "            grabbed.append(years[i*(36//n_rows)])\n",
    "\n",
    "        #2. Pool together into new dataframe and use knn imputation\n",
    "        df_for_knn_imputation = filled_indicators_df.loc[(slice(None), (grabbed)), :]\n",
    "        \n",
    "        #Check for all null columns (non imputable)\n",
    "        if(df_for_knn_imputation.isnull().all().any() == True):\n",
    "            print('----------- *! Warning !* -------------')\n",
    "            print('Some columns have all nan values, fetching another few rows')\n",
    "            print('total rows: ', df_for_knn_imputation.shape[0])\n",
    "            d[n_rows] = df_for_knn_imputation.isnull().sum(axis = 0)\n",
    "            \n",
    "            #find which columns have all nan values\n",
    "            #print(\"Count: \")\n",
    "            #print(df_for_knn_imputation.isnull().sum(axis = 0))\n",
    "            #nan_count = df_for_knn_imputation.isnull().sum(axis = 0)\n",
    "            #print(nan_count[nan_count == ])\n",
    "            #df_for_knn_imputation[df_for_knn_imputation.isnull().sum(axis = 0) == df_for_knn_imputation.shape[0]]\n",
    "\n",
    "        else:\n",
    "            checker = False\n",
    "    \n",
    "    print('Sucess: no columns have all nan values')\n",
    "    print(\"Years: \", grabbed)\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors = n_knn_neighbors, weights = knn_weights)\n",
    "    features_filled_knn = imputer.fit_transform(df_for_knn_imputation)\n",
    "\n",
    "    #3. Return to orignal df\n",
    "    \n",
    "    #4. perform time interpolation\n",
    "\n",
    "    return features_filled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---------------------------\n",
    "\n",
    "\n",
    "# IMPUTATION\n",
    "Using TIME INTERPOLATION AND ITERATIVE IMP\n",
    "\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_and_impute(dataframe, method = 'iterative', max_iter = 5, n_nearest_features = None):\n",
    "    \n",
    "    '''\n",
    "    Use time interpolation and either iterative or knn interpolation to fill missing values in df\n",
    "    \n",
    "    args:\n",
    "        dataframe -- df to be imputed\n",
    "        method    -- iterative or knn (iter by default)\n",
    "        max_iter  -- how many iterations of iterative imputation to perform\n",
    "        n_nearest_features -- how many features to use for multivariate iter imp, by default use all of them\n",
    "        \n",
    "    returns:\n",
    "        df_imputed\n",
    "    '''\n",
    "    \n",
    "    #this comes from the original csv read into variable 'data'\n",
    "    countries = data['Country Name'].unique()\n",
    "    \n",
    "    #by default, use all of the features for iterative imp\n",
    "    if(n_nearest_features == None):\n",
    "        n_nearest_features = int(dataframe.shape[1])\n",
    "\n",
    "    #This splits the unimputed df into seperate DFs by country, then interpolates them\n",
    "    forward_interpolated_dict = {\"X_features_filled_unimputed\": dataframe}   #first element is unimputed df\n",
    "    for country in countries:\n",
    "        forward_interpolated_dict[country] = dataframe.loc[country].interpolate(method = 'time', axis = 0, limit_direction = 'both')\n",
    "\n",
    "    #make sure country_col is correct length\n",
    "    range_amount = len(forward_interpolated_dict['Afghanistan'].index)\n",
    "\n",
    "    #append country name column for later regrouping by country (Afghanistan outside to have a starting df to concat to)\n",
    "    country_col = ['Afghanistan' for x in range(range_amount)]\n",
    "    interpolated_indicators_restitched = forward_interpolated_dict['Afghanistan']\n",
    "    interpolated_indicators_restitched['Country Name'] = country_col\n",
    "    \n",
    "    #TODO WArning: Make sure concat isn't creating any new NAN values 1    1  nan  nan\n",
    "    #                                                                nan  nan  1    1\n",
    "    \n",
    "\n",
    "    #add country name column foreach country, then concat to restitched df\n",
    "    for item in forward_interpolated_dict:\n",
    "        if(item != 'Afghanistan' and item != 'X_features_filled_unimputed'):\n",
    "            to_be_concat = forward_interpolated_dict[item]\n",
    "            to_be_concat['Country Name'] = [item for x in range(range_amount)]\n",
    "            interpolated_indicators_restitched = pd.concat([interpolated_indicators_restitched, to_be_concat], axis = 0)\n",
    "\n",
    "    #Recreate the original multiindex dataframe (ie X_features_filled) with interpolated values \n",
    "    index = interpolated_indicators_restitched.index\n",
    "    country_col_all = interpolated_indicators_restitched['Country Name']\n",
    "    interpolated_indicators_restitched.drop(columns = 'Country Name', inplace = True)\n",
    "     \n",
    "    if(method == 'iterative'):\n",
    "        imputer = IterativeImputer(max_iter = max_iter, n_nearest_features = n_nearest_features)\n",
    "        \n",
    "    elif (method == 'knn'):\n",
    "        imputer = KNNImputer()\n",
    "    \n",
    "    imputer.fit(interpolated_indicators_restitched)\n",
    "    dataframe_fully_interpd = imputer.transform(interpolated_indicators_restitched)\n",
    "    dataframe_fully_interpd = pd.DataFrame(dataframe_fully_interpd, columns = dataframe.columns, index = index)\n",
    "    dataframe_fully_interpd['Country Name'] = country_col_all\n",
    "    dataframe_fully_interpd = dataframe_fully_interpd.set_index(['Country Name', index])\n",
    "\n",
    "    return dataframe_fully_interpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_test_val(dataframe, method = 'iterative', filler = 0):\n",
    "    '''\n",
    "    Use either iterative or knn interpolation to fill missing values in test or validation df\n",
    "    \n",
    "    args:\n",
    "        dataframe -- df to be imputed\n",
    "        method    -- iterative or knn (iter by default)\n",
    "        filler    -- replace all-nan columns with this value\n",
    "        \n",
    "        TODO: maybe find a bettersolution to the filler value\n",
    "        \n",
    "    returns:\n",
    "        df_imputed\n",
    "    '''\n",
    "    \n",
    "    #find all-nan columns\n",
    "    all_nan_cols_bool = dataframe.isnull().all().tolist()\n",
    "    all_nan_cols = list(dataframe.columns[all_nan_cols_bool])\n",
    "    \n",
    "    #replace these columns with filler values\n",
    "    dataframe[all_nan_cols] = dataframe[all_nan_cols].fillna(value = filler)\n",
    "    \n",
    "    #and now impute\n",
    "    \n",
    "    if (method == 'iterative'):\n",
    "        imputer = IterativeImputer()\n",
    "    \n",
    "    elif(method == 'knn'):\n",
    "        imputer = KNNImputer()\n",
    "    \n",
    "    imputer.fit(dataframe)\n",
    "    imputed_dataframe = imputer.transform(dataframe)\n",
    "    \n",
    "    return pd.DataFrame(imputed_dataframe, columns = dataframe.columns, index = dataframe.index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# MODEL EVALUATION HELPER FUNCTIONS\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unimputed_cost(y_unimputed, y_hat, y_test_imputed, error_type):\n",
    "    '''\n",
    "    Get boolean array of non-nan values from the unimputed target df (y_unimputed), access corresponding \n",
    "    elements from y_hat and y_test, and compare these two df's, giving the actual performance\n",
    "    \n",
    "    args:\n",
    "        y_unimputed -- target dataframe before imputing\n",
    "        y_hat -- predicted target array \n",
    "        y_test_imputed -- from train test split (imputed)\n",
    "        error_type -- mean squared, absolute, etc\n",
    "    \n",
    "    returns:\n",
    "        cost \n",
    "    '''\n",
    "    \n",
    "    #This is for naiive forecsting since y_hat_naiive won't be a numpy array, but y_hat from models will be\n",
    "    if(type(y_hat) != np.ndarray):\n",
    "        y_hat = y_hat.values\n",
    "    \n",
    "    bool_array = y_unimputed.isna()\n",
    "    y_hat_filtered = y_hat[bool_array]\n",
    "    y_test_filtered = y_test_imputed.values[bool_array]\n",
    "    \n",
    "    assert(y_hat_filtered.shape == y_test_filtered.shape)\n",
    "    \n",
    "    cost = 0\n",
    "    \n",
    "    if(error_type == 'absolute'):\n",
    "        cost = mean_absolute_error(y_hat_filtered, y_test_filtered)\n",
    "    \n",
    "    elif(error_type == 'squared'):\n",
    "        cost = mean_squared_error(y_hat_filtered, y_test_filtered)\n",
    "    \n",
    "    return cost, y_hat_filtered, y_test_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# PIPELINE\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                        Step 0: A - Get targets and indicators in seperate dataframes\n",
    "                                            and format indices\n",
    "\n",
    "                                B - Fill target dataframe with missing targets for\n",
    "                                                each country\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "#A\n",
    "\n",
    "#Pull data from original csv and seperate mdgs from indicators\n",
    "mdgs = submission.index\n",
    "mdgs_for_i2g_model = data_original.loc[mdgs]\n",
    "\n",
    "target_names = mdgs_for_i2g_model['Series Name'].unique()\n",
    "indicators_for_i2g_model = data_original[data_original['Series Name'].isin(target_names) == False]\n",
    "indicators_for_i2g_model.sort_values(by = ['Country Name', 'Series Name'])\n",
    "\n",
    "#Get data_original into proper DataFrame format for i2g \n",
    "\n",
    "#Create a multi index DataFrame for legibility\n",
    "X_features = indicators_for_i2g_model.set_index(['Country Name', 'Series Name'])\n",
    "X_features = X_features.drop(['Series Code'], axis = 1)\n",
    "X_features = X_features.sort_index()\n",
    "\n",
    "#make sure there is no overlap between mdgs and indicators\n",
    "assert(set(X_features.index.get_level_values(1)) & set(mdgs_for_i2g_model['Series Name'].values) == set())\n",
    "\n",
    "\n",
    "#B\n",
    "\n",
    "#create a goals df\n",
    "targets = mdgs_for_i2g_model.set_index(['Country Name', 'Series Name'])\n",
    "targets = targets.drop('Series Code', axis = 1)\n",
    "\n",
    "#!NOTE! - targets does not contain all countries as the submission does not require goal predictions for every country\n",
    "# try adding empties for missing countries, then try again and disregard missing countries\n",
    "\n",
    "#214 countries in the dataset\n",
    "countries = data['Country Name'].unique()\n",
    "\n",
    "countries_in_targets = targets.index.unique()\n",
    "missing_countries = set(countries) - set(countries_in_targets)\n",
    "\n",
    "years = X_features.columns\n",
    "\n",
    "nan_row = np.empty((1, len(years)))\n",
    "nan_row[:] = np.nan\n",
    "\n",
    "#for missing countries\n",
    "not_found = False\n",
    "\n",
    "#add missing targets to each country (this is similar enough to what I did for X_features missing indicators. maybe this can be a function?\n",
    "for country in countries:\n",
    "    \n",
    "    #get current country targets (this might fail if country isn't in mdgs)\n",
    "    try: \n",
    "        current = targets.loc[country]\n",
    "    except:\n",
    "        not_found = True\n",
    "\n",
    "    #which targets are missing?\n",
    "    if(not_found == False):\n",
    "        missing_targets = set(target_names) - set(current.index.tolist())\n",
    "    \n",
    "    #country wasn't in mdgs, so its missing all targets\n",
    "    else:\n",
    "        missing_targets = target_names\n",
    "        \n",
    "    #create nan dataframe for missing targets and append to targets df\n",
    "    missing_targets_indices = [np.array([country for x in range(len(missing_targets))]), np.array(list(missing_targets))]\n",
    "\n",
    "    nan_matrix = np.empty((len(missing_targets), len(years)))\n",
    "    nan_matrix[:] = np.nan\n",
    "\n",
    "    missing_targets_midf = pd.DataFrame(nan_matrix, index = missing_targets_indices, columns = years)\n",
    "    targets = targets.append(missing_targets_midf)\n",
    "    not_found = False\n",
    "    \n",
    "#fix append issue (so added vectors appear with grouped country)\n",
    "targets = targets.sort_index()\n",
    "\n",
    "#reformat years so they are in datetime format\n",
    "reformatted_cols = targets.columns.map(convert_to_datetime).tolist()\n",
    "targets.columns = reformatted_cols\n",
    "\n",
    "#essentially swapping the years columns with the 'Series Name' index. In a sense, transposing\n",
    "targets_stacked = targets.stack(dropna = False)\n",
    "targets = targets_stacked.unstack(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of indicators: 80\n"
     ]
    }
   ],
   "source": [
    "'''-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                        Step 1: get only indicators from each country that are in \n",
    "                                        the most_common_indicator_list\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "most_common_indicators_list = get_indicators_for_model(200, indicators_for_i2g_model)\n",
    "X_features_most_common = X_features[X_features.index.isin(most_common_indicators_list, level=1)]\n",
    "\n",
    "print('Number of indicators: %d' % len(most_common_indicators_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                        Step 2: fill indicator dataframe with missing indicators\n",
    "                                            for each country\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "X_features_filled = add_missing_indicators(X_features_most_common, most_common_indicators_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                             Step 2.01: Normalize/Standardize the features\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "columns = X_features_filled.columns\n",
    "index = X_features_filled.index\n",
    "\n",
    "#Normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_features_filled = scaler.fit_transform(X_features_filled)\n",
    "\n",
    "#Standardization\n",
    "#scaler = StandardScaler()\n",
    "#X_features_filled = scaler.fit_transform(X_features_filled)\n",
    "\n",
    "X_features_filled = pd.DataFrame(X_features_filled, columns = columns, index = index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 relevant features chosen\n"
     ]
    }
   ],
   "source": [
    "'''-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                        Step 2.1: Pearson Correlation for Feature Selection\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "#This is calculated here in order to avoid leakage\n",
    "train_indices = X_features_filled.loc['Afghanistan'].index[0:34].tolist()\n",
    "\n",
    "relevance_dict = {}\n",
    "correlation_threshold = 0.75\n",
    "features_and_targets = pd.concat([X_features_filled.loc[pd.IndexSlice[:, train_indices], :], targets.loc[pd.IndexSlice[:, train_indices], :]], axis = 1, sort = False)\n",
    "\n",
    "#Correlation to targets\n",
    "cor = features_and_targets.corr()\n",
    "\n",
    "#add each goal's correlated features to the relevance_dict\n",
    "for i in range(7):    \n",
    "    cor_target = abs(cor[target_names[i]])\n",
    "    relevant = cor_target[cor_target >= correlation_threshold]\n",
    "    relevance_dict[target_names[i]] = relevant\n",
    "\n",
    "#TODO: Caution, Malaria Goal has 1.0 correlation with indicators that it maybe shouldn't \n",
    "\n",
    "#create a set of relevant features from each of the targets' correlation matrices\n",
    "relevant_features = set([])\n",
    "\n",
    "for goal in relevance_dict.keys():\n",
    "    #print(\"\\n----------  Relevant Features for: \", goal + ' -------------\\n')\n",
    "    #print(relevance_dict[goal])\n",
    "    relevant_features.update(relevance_dict[goal].index.tolist())\n",
    "\n",
    "#remove targets from relevant features\n",
    "relevant_features = relevant_features - set(target_names)\n",
    "\n",
    "print('%d relevant features chosen' % len(relevant_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series Name</th>\n",
       "      <th>Adjusted savings: consumption of fixed capital (current US$)</th>\n",
       "      <th>Adjusted savings: energy depletion (% of GNI)</th>\n",
       "      <th>Adjusted savings: energy depletion (current US$)</th>\n",
       "      <th>Adjusted savings: mineral depletion (% of GNI)</th>\n",
       "      <th>Adjusted savings: mineral depletion (current US$)</th>\n",
       "      <th>Agricultural land (% of land area)</th>\n",
       "      <th>Agricultural land (sq. km)</th>\n",
       "      <th>Arable land (% of land area)</th>\n",
       "      <th>Arable land (hectares per person)</th>\n",
       "      <th>Arable land (hectares)</th>\n",
       "      <th>...</th>\n",
       "      <th>Urban population</th>\n",
       "      <th>Urban population (% of total)</th>\n",
       "      <th>Urban population growth (annual %)</th>\n",
       "      <th>Achieve universal primary education</th>\n",
       "      <th>Combat HIV/AIDS</th>\n",
       "      <th>Combat malaria and other diseases</th>\n",
       "      <th>Develop a global partnership for development: Internet Use</th>\n",
       "      <th>Ensure environmental sustainability</th>\n",
       "      <th>Improve maternal health</th>\n",
       "      <th>Reduce child mortality</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Afghanistan</th>\n",
       "      <th>1972-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638538</td>\n",
       "      <td>0.072158</td>\n",
       "      <td>0.165239</td>\n",
       "      <td>0.220347</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>0.094707</td>\n",
       "      <td>0.782999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015293</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638572</td>\n",
       "      <td>0.072162</td>\n",
       "      <td>0.165239</td>\n",
       "      <td>0.214417</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.099326</td>\n",
       "      <td>0.779651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030615</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638572</td>\n",
       "      <td>0.072162</td>\n",
       "      <td>0.165239</td>\n",
       "      <td>0.209048</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.103946</td>\n",
       "      <td>0.774461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029049</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638572</td>\n",
       "      <td>0.072162</td>\n",
       "      <td>0.165239</td>\n",
       "      <td>0.204414</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.108565</td>\n",
       "      <td>0.767797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025262</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638572</td>\n",
       "      <td>0.072162</td>\n",
       "      <td>0.165239</td>\n",
       "      <td>0.200342</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.113493</td>\n",
       "      <td>0.765775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Zimbabwe</th>\n",
       "      <th>2001-01-01</th>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.006976</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.430027</td>\n",
       "      <td>0.028903</td>\n",
       "      <td>0.126085</td>\n",
       "      <td>0.092233</td>\n",
       "      <td>0.018965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007218</td>\n",
       "      <td>0.323405</td>\n",
       "      <td>0.707245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007998</td>\n",
       "      <td>0.796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-01</th>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.014633</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.436574</td>\n",
       "      <td>0.029339</td>\n",
       "      <td>0.127846</td>\n",
       "      <td>0.093121</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007339</td>\n",
       "      <td>0.327726</td>\n",
       "      <td>0.703273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039944</td>\n",
       "      <td>0.796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-01</th>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.035144</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.441697</td>\n",
       "      <td>0.029681</td>\n",
       "      <td>0.127846</td>\n",
       "      <td>0.092884</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007447</td>\n",
       "      <td>0.332048</td>\n",
       "      <td>0.700235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063948</td>\n",
       "      <td>0.796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-01</th>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.011294</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.068522</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.451659</td>\n",
       "      <td>0.030345</td>\n",
       "      <td>0.133130</td>\n",
       "      <td>0.096572</td>\n",
       "      <td>0.020025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.336369</td>\n",
       "      <td>0.698453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-01</th>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.008375</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.070292</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.454505</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>0.136652</td>\n",
       "      <td>0.098992</td>\n",
       "      <td>0.020555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.340691</td>\n",
       "      <td>0.697921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.080160</td>\n",
       "      <td>0.797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7276 rows  87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Series Name              Adjusted savings: consumption of fixed capital (current US$)  \\\n",
       "Country Name                                                                            \n",
       "Afghanistan  1972-01-01                                                NaN              \n",
       "             1973-01-01                                                NaN              \n",
       "             1974-01-01                                                NaN              \n",
       "             1975-01-01                                                NaN              \n",
       "             1976-01-01                                                NaN              \n",
       "...                                                                    ...              \n",
       "Zimbabwe     2001-01-01                                           0.000405              \n",
       "             2002-01-01                                           0.000378              \n",
       "             2003-01-01                                           0.000339              \n",
       "             2004-01-01                                           0.000343              \n",
       "             2005-01-01                                           0.000329              \n",
       "\n",
       "Series Name              Adjusted savings: energy depletion (% of GNI)  \\\n",
       "Country Name                                                             \n",
       "Afghanistan  1972-01-01                                       0.013971   \n",
       "             1973-01-01                                       0.015293   \n",
       "             1974-01-01                                       0.030615   \n",
       "             1975-01-01                                       0.029049   \n",
       "             1976-01-01                                       0.025262   \n",
       "...                                                                ...   \n",
       "Zimbabwe     2001-01-01                                       0.003995   \n",
       "             2002-01-01                                       0.001098   \n",
       "             2003-01-01                                       0.001823   \n",
       "             2004-01-01                                       0.011294   \n",
       "             2005-01-01                                       0.008375   \n",
       "\n",
       "Series Name              Adjusted savings: energy depletion (current US$)  \\\n",
       "Country Name                                                                \n",
       "Afghanistan  1972-01-01                                          0.000104   \n",
       "             1973-01-01                                          0.000124   \n",
       "             1974-01-01                                          0.000309   \n",
       "             1975-01-01                                          0.000322   \n",
       "             1976-01-01                                          0.000302   \n",
       "...                                                                   ...   \n",
       "Zimbabwe     2001-01-01                                          0.000118   \n",
       "             2002-01-01                                          0.000031   \n",
       "             2003-01-01                                          0.000046   \n",
       "             2004-01-01                                          0.000286   \n",
       "             2005-01-01                                          0.000210   \n",
       "\n",
       "Series Name              Adjusted savings: mineral depletion (% of GNI)  \\\n",
       "Country Name                                                              \n",
       "Afghanistan  1972-01-01                                        0.000000   \n",
       "             1973-01-01                                        0.000000   \n",
       "             1974-01-01                                        0.000000   \n",
       "             1975-01-01                                        0.000000   \n",
       "             1976-01-01                                        0.000000   \n",
       "...                                                                 ...   \n",
       "Zimbabwe     2001-01-01                                        0.006976   \n",
       "             2002-01-01                                        0.014633   \n",
       "             2003-01-01                                        0.035144   \n",
       "             2004-01-01                                        0.068522   \n",
       "             2005-01-01                                        0.070292   \n",
       "\n",
       "Series Name              Adjusted savings: mineral depletion (current US$)  \\\n",
       "Country Name                                                                 \n",
       "Afghanistan  1972-01-01                                           0.000000   \n",
       "             1973-01-01                                           0.000000   \n",
       "             1974-01-01                                           0.000000   \n",
       "             1975-01-01                                           0.000000   \n",
       "             1976-01-01                                           0.000000   \n",
       "...                                                                    ...   \n",
       "Zimbabwe     2001-01-01                                           0.000263   \n",
       "             2002-01-01                                           0.000520   \n",
       "             2003-01-01                                           0.001132   \n",
       "             2004-01-01                                           0.002212   \n",
       "             2005-01-01                                           0.002252   \n",
       "\n",
       "Series Name              Agricultural land (% of land area)  \\\n",
       "Country Name                                                  \n",
       "Afghanistan  1972-01-01                            0.638538   \n",
       "             1973-01-01                            0.638572   \n",
       "             1974-01-01                            0.638572   \n",
       "             1975-01-01                            0.638572   \n",
       "             1976-01-01                            0.638572   \n",
       "...                                                     ...   \n",
       "Zimbabwe     2001-01-01                            0.430027   \n",
       "             2002-01-01                            0.436574   \n",
       "             2003-01-01                            0.441697   \n",
       "             2004-01-01                            0.451659   \n",
       "             2005-01-01                            0.454505   \n",
       "\n",
       "Series Name              Agricultural land (sq. km)  \\\n",
       "Country Name                                          \n",
       "Afghanistan  1972-01-01                    0.072158   \n",
       "             1973-01-01                    0.072162   \n",
       "             1974-01-01                    0.072162   \n",
       "             1975-01-01                    0.072162   \n",
       "             1976-01-01                    0.072162   \n",
       "...                                             ...   \n",
       "Zimbabwe     2001-01-01                    0.028903   \n",
       "             2002-01-01                    0.029339   \n",
       "             2003-01-01                    0.029681   \n",
       "             2004-01-01                    0.030345   \n",
       "             2005-01-01                    0.030534   \n",
       "\n",
       "Series Name              Arable land (% of land area)  \\\n",
       "Country Name                                            \n",
       "Afghanistan  1972-01-01                      0.165239   \n",
       "             1973-01-01                      0.165239   \n",
       "             1974-01-01                      0.165239   \n",
       "             1975-01-01                      0.165239   \n",
       "             1976-01-01                      0.165239   \n",
       "...                                               ...   \n",
       "Zimbabwe     2001-01-01                      0.126085   \n",
       "             2002-01-01                      0.127846   \n",
       "             2003-01-01                      0.127846   \n",
       "             2004-01-01                      0.133130   \n",
       "             2005-01-01                      0.136652   \n",
       "\n",
       "Series Name              Arable land (hectares per person)  \\\n",
       "Country Name                                                 \n",
       "Afghanistan  1972-01-01                           0.220347   \n",
       "             1973-01-01                           0.214417   \n",
       "             1974-01-01                           0.209048   \n",
       "             1975-01-01                           0.204414   \n",
       "             1976-01-01                           0.200342   \n",
       "...                                                    ...   \n",
       "Zimbabwe     2001-01-01                           0.092233   \n",
       "             2002-01-01                           0.093121   \n",
       "             2003-01-01                           0.092884   \n",
       "             2004-01-01                           0.096572   \n",
       "             2005-01-01                           0.098992   \n",
       "\n",
       "Series Name              Arable land (hectares)  ...  Urban population  \\\n",
       "Country Name                                     ...                     \n",
       "Afghanistan  1972-01-01                0.041905  ...          0.002329   \n",
       "             1973-01-01                0.041905  ...          0.002484   \n",
       "             1974-01-01                0.041905  ...          0.002640   \n",
       "             1975-01-01                0.041905  ...          0.002795   \n",
       "             1976-01-01                0.041905  ...          0.002955   \n",
       "...                                         ...  ...               ...   \n",
       "Zimbabwe     2001-01-01                0.018965  ...          0.007218   \n",
       "             2002-01-01                0.019230  ...          0.007339   \n",
       "             2003-01-01                0.019230  ...          0.007447   \n",
       "             2004-01-01                0.020025  ...          0.007548   \n",
       "             2005-01-01                0.020555  ...          0.007648   \n",
       "\n",
       "Series Name              Urban population (% of total)  \\\n",
       "Country Name                                             \n",
       "Afghanistan  1972-01-01                       0.094707   \n",
       "             1973-01-01                       0.099326   \n",
       "             1974-01-01                       0.103946   \n",
       "             1975-01-01                       0.108565   \n",
       "             1976-01-01                       0.113493   \n",
       "...                                                ...   \n",
       "Zimbabwe     2001-01-01                       0.323405   \n",
       "             2002-01-01                       0.327726   \n",
       "             2003-01-01                       0.332048   \n",
       "             2004-01-01                       0.336369   \n",
       "             2005-01-01                       0.340691   \n",
       "\n",
       "Series Name              Urban population growth (annual %)  \\\n",
       "Country Name                                                  \n",
       "Afghanistan  1972-01-01                            0.782999   \n",
       "             1973-01-01                            0.779651   \n",
       "             1974-01-01                            0.774461   \n",
       "             1975-01-01                            0.767797   \n",
       "             1976-01-01                            0.765775   \n",
       "...                                                     ...   \n",
       "Zimbabwe     2001-01-01                            0.707245   \n",
       "             2002-01-01                            0.703273   \n",
       "             2003-01-01                            0.700235   \n",
       "             2004-01-01                            0.698453   \n",
       "             2005-01-01                            0.697921   \n",
       "\n",
       "Series Name              Achieve universal primary education  Combat HIV/AIDS  \\\n",
       "Country Name                                                                    \n",
       "Afghanistan  1972-01-01                                  NaN              NaN   \n",
       "             1973-01-01                                  NaN              NaN   \n",
       "             1974-01-01                                  NaN              NaN   \n",
       "             1975-01-01                                  NaN              NaN   \n",
       "             1976-01-01                                  NaN              NaN   \n",
       "...                                                      ...              ...   \n",
       "Zimbabwe     2001-01-01                                  NaN            0.243   \n",
       "             2002-01-01                                  NaN            0.228   \n",
       "             2003-01-01                                  NaN            0.213   \n",
       "             2004-01-01                                  NaN            0.198   \n",
       "             2005-01-01                                  NaN            0.184   \n",
       "\n",
       "Series Name              Combat malaria and other diseases  \\\n",
       "Country Name                                                 \n",
       "Afghanistan  1972-01-01                                NaN   \n",
       "             1973-01-01                                NaN   \n",
       "             1974-01-01                                NaN   \n",
       "             1975-01-01                                NaN   \n",
       "             1976-01-01                                NaN   \n",
       "...                                                    ...   \n",
       "Zimbabwe     2001-01-01                                NaN   \n",
       "             2002-01-01                                NaN   \n",
       "             2003-01-01                                NaN   \n",
       "             2004-01-01                                NaN   \n",
       "             2005-01-01                                NaN   \n",
       "\n",
       "Series Name              Develop a global partnership for development: Internet Use  \\\n",
       "Country Name                                                                          \n",
       "Afghanistan  1972-01-01                                                NaN            \n",
       "             1973-01-01                                                NaN            \n",
       "             1974-01-01                                                NaN            \n",
       "             1975-01-01                                                NaN            \n",
       "             1976-01-01                                                NaN            \n",
       "...                                                                    ...            \n",
       "Zimbabwe     2001-01-01                                           0.007998            \n",
       "             2002-01-01                                           0.039944            \n",
       "             2003-01-01                                           0.063948            \n",
       "             2004-01-01                                           0.065640            \n",
       "             2005-01-01                                           0.080160            \n",
       "\n",
       "Series Name              Ensure environmental sustainability  \\\n",
       "Country Name                                                   \n",
       "Afghanistan  1972-01-01                                  NaN   \n",
       "             1973-01-01                                  NaN   \n",
       "             1974-01-01                                  NaN   \n",
       "             1975-01-01                                  NaN   \n",
       "             1976-01-01                                  NaN   \n",
       "...                                                      ...   \n",
       "Zimbabwe     2001-01-01                                0.796   \n",
       "             2002-01-01                                0.796   \n",
       "             2003-01-01                                0.796   \n",
       "             2004-01-01                                0.796   \n",
       "             2005-01-01                                0.797   \n",
       "\n",
       "Series Name              Improve maternal health  Reduce child mortality  \n",
       "Country Name                                                              \n",
       "Afghanistan  1972-01-01                      NaN                  0.2960  \n",
       "             1973-01-01                      NaN                  0.2909  \n",
       "             1974-01-01                      NaN                  0.2852  \n",
       "             1975-01-01                      NaN                  0.2798  \n",
       "             1976-01-01                      NaN                  0.2742  \n",
       "...                                          ...                     ...  \n",
       "Zimbabwe     2001-01-01                      NaN                  0.1014  \n",
       "             2002-01-01                      NaN                  0.1001  \n",
       "             2003-01-01                      NaN                  0.0985  \n",
       "             2004-01-01                      NaN                  0.0973  \n",
       "             2005-01-01                      NaN                  0.0966  \n",
       "\n",
       "[7276 rows x 87 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_and_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                             Step 2.2: Training/Val/Test Split\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------'''\n",
    "#TODO Target leakage if Val/Test set used in correlation?\n",
    "\n",
    "#update X_features_filled with relevant features\n",
    "X_features_filled_relevant = X_features_filled[relevant_features]\n",
    "\n",
    "#training set\n",
    "train_indices_strings = list(map(lambda x: str(train_indices[x]), range(len(train_indices))))\n",
    "X_train = X_features_filled_relevant.loc[pd.IndexSlice[:, train_indices], :]\n",
    "y_train = targets.loc[pd.IndexSlice[:, train_indices], :]\n",
    "\n",
    "#val set\n",
    "valid_year = '2006-01-01'\n",
    "X_valid = X_features_filled_relevant.loc[pd.IndexSlice[:, [valid_year]], :]\n",
    "y_valid = targets.loc[pd.IndexSlice[:, [valid_year]], :]\n",
    "\n",
    "#test set\n",
    "test_year = '2007-01-01'\n",
    "X_test = X_features_filled_relevant.loc[pd.IndexSlice[:, [test_year]], :]\n",
    "y_test = targets.loc[pd.IndexSlice[:, [test_year]], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing X_train . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:670: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train imputation successful! \n",
      " Imputing y_train . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:670: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Imputation Complete!\n",
      "Imputing Validation and Test Sets . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:670: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n",
      "C:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:670: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n",
      "C:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:670: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation and Test Sets Imputed Successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yousef\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:670: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "'''-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                                Step 2.3: IMPUTATION OF SETS\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "#Iterative Imputer parameters\n",
    "y_n_iter = 10\n",
    "x_n_iter = 3\n",
    "\n",
    "print('Imputing X_train . . .')\n",
    "X_train_imputed = interpolate_and_impute(X_train, max_iter = x_n_iter)\n",
    "\n",
    "print('X_train imputation successful! \\n Imputing y_train . . .')\n",
    "y_train_imputed = interpolate_and_impute(y_train, max_iter = y_n_iter)\n",
    "\n",
    "print('Training Set Imputation Complete!')\n",
    "\n",
    "#TODO: For now, run iterative imputer on test and val sets until they converge since they are pretty small\n",
    "\n",
    "print('Imputing Validation and Test Sets . . .')\n",
    "X_val_imputed, X_test_imputed = impute_test_val(X_valid), impute_test_val(X_test)\n",
    "y_val_imputed, y_test_imputed = impute_test_val(y_valid), impute_test_val(y_test)\n",
    "\n",
    "#Make sure imputation was successful\n",
    "assert(y_train_imputed.isnull().any().any() == False)\n",
    "assert(X_train_imputed.isnull().any().any() == False)\n",
    "assert(X_val_imputed.isnull().any().any() == False)\n",
    "assert(X_test_imputed.isnull().any().any() == False)\n",
    "assert(y_val_imputed.isnull().any().any() == False)\n",
    "assert(y_test_imputed.isnull().any().any() == False)\n",
    "\n",
    "print('Validation and Test Sets Imputed Successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# TABLEAU\n",
    "\n",
    "EXPORT DF TO EXCEL FOR TABLEAU DATa VIZ\n",
    "                                \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reset = X_features_filled.reset_index()\n",
    "X_reset = X_reset.set_index('Country Name')\n",
    "X_reset.rename(columns = {'level_1':'Year'}, inplace = True)\n",
    "X_reset.to_excel(r'C:\\Users\\Yousef\\Desktop\\export_df_3.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Modelling \n",
    "\n",
    "In this section I will develop models to forecast the UNMDGS (Targets) for each country. I will Use the years 2006 and 2007 for training and validation in order to fine tune my models, but the aim of the project is to forecast values for 2008 and 2012.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: \n",
    "Naive Forecasting: Use Last Year's Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series Name\n",
       "Achieve universal primary education                           0.945748\n",
       "Combat HIV/AIDS                                               0.012538\n",
       "Combat malaria and other diseases                             0.025115\n",
       "Develop a global partnership for development: Internet Use    0.060439\n",
       "Ensure environmental sustainability                           0.962000\n",
       "Improve maternal health                                       0.000285\n",
       "Reduce child mortality                                        0.022200\n",
       "Name: (Albania, 2005-01-01 00:00:00), dtype: float64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Baseline MSE for 2006 Target Predictions: 0.001493 \n",
      " Baseline unimputed MSE for 2006 Target Prediction 0.003208\n",
      "\n",
      "Baseline MSE for 2007 Target Predictions: 0.000919 \n",
      " Baseline unimputed MSE for 2007 Target Prediction 0.000735\n",
      "\n",
      "----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#X_features_filled_relevant.loc[pd.IndexSlice[:, train_indices], :]\n",
    "y_hat_2006_naiive = y_train_imputed.loc[pd.IndexSlice[:, '2005'], :]\n",
    "mse_2006_naiive = mean_squared_error(y_hat_2006_naiive.values, y_val_imputed.values)\n",
    "mse_2006_naiive_unimputed, _, _ = compute_unimputed_cost(y_valid, y_hat_2006_naiive, y_val_imputed, 'squared')\n",
    "\n",
    "y_hat_2007_naiive = y_val_imputed\n",
    "mse_2007_naiive = mean_squared_error(y_hat_2007_naiive.values, y_test_imputed.values)\n",
    "mse_2007_naiive_unimputed, _, _ = compute_unimputed_cost(y_test, y_hat_2007_naiive, y_test_imputed, 'squared')\n",
    "\n",
    "print('\\n----------------------------------------------------\\n')\n",
    "print('Baseline MSE for 2006 Target Predictions: %f \\n Baseline unimputed MSE for 2006 Target Prediction %f' %(mse_2006_naiive, mse_2006_naiive_unimputed))\n",
    "print('\\nBaseline MSE for 2007 Target Predictions: %f \\n Baseline unimputed MSE for 2007 Target Prediction %f' % (mse_2007_naiive, mse_2007_naiive_unimputed))\n",
    "print('\\n----------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Use models for forecsting the indicators for the following year, then\n",
    "    feed all of them into one I2G model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (On Hold, CNN is current priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape = (X_fcc.shape[1], 1)))\n",
    "model.add(Dense(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape inputs to 3D [samples, timestamps, features] = [no countries, years in set (34 for training), relevant features]\n",
    "#X_train_imputed_reshaped = X_train_imputed.values.reshape((len(countries), len(train_indices), X_train_imputed.shape[1]))\n",
    "#X_val_imputed_reshaped = X_train_imputed.values.reshape((len(countries), 1, X_train_imputed.shape[1]))\n",
    "#X_test_imputed_reshaped = X_train_imputed.values.reshape((len(countries), 1, X_train_imputed.shape[1]))\n",
    "\n",
    "#Network Archtecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(50))#, input_shape = (X_train_imputed_reshaped[1], X_train_imputed_reshaped[2]))\n",
    "model.add(Dense(7))\n",
    "model.compile(loss = 'mae', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD: 1D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: Multivariate Forecasting -> Goal Prediction\n",
    "This model acts as a baseline for the next CNN models. It takes 34-year samples for each countrys indicators, and predicts the values for the indicators the following year, then predicts the UNMDGs for that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Forecast Goals From Previous Years' Indicators\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 34, 71)            22223     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 8, 71)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 568)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 780)               443820    \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 78)                60918     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 7)                 553       \n",
      "=================================================================\n",
      "Total params: 533,676\n",
      "Trainable params: 533,676\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.name = \"Forecast Goals From Previous Years' Indicators\"\n",
    "model.name\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_hyperparam_to_loss_dict = {'params':('filters', 'ksize', 'psize', 'epochs', 'd1out')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for reproducible results\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "#Hyperparams\n",
    "n_filters = int(X_train_with_goals.shape[1])\n",
    "kernel_size = 7\n",
    "pool_size = 3\n",
    "epochs = 120\n",
    "\n",
    "#input dims\n",
    "n_samples = 214\n",
    "n_steps = X_train_imputed.loc['Afghanistan'].shape[0]\n",
    "#n_features = X_train_imputed.shape[1]\n",
    "n_features = X_train_with_goals.shape[1]\n",
    "\n",
    "dense_1_output = int(n_features * 10) #For handling vector from convolution\n",
    "dense_2_output = int(n_features)  #Forecast next years indicators \n",
    "dense_3_output = int(n_features)  #Hidden Layer\n",
    "\n",
    "#predicting all goals with same model: 7\n",
    "dense_4_output = 7\n",
    "\n",
    "#shape of inputs(no_countries x n_years x n_relevant_features)\n",
    "x_shape = (n_samples, n_steps, n_features)\n",
    "\n",
    "#shape of outputs(no_countries x no_goals)\n",
    "y_shape = (n_samples, 7)\n",
    "\n",
    "#for tuning\n",
    "h_params = [n_filters, kernel_size, pool_size, epochs, dense_1_output]\n",
    "\n",
    "#reshape data\n",
    "#X_train_imputed_reshaped_CNN = np.array(X_train_imputed.values).reshape((n_samples, n_steps, n_features))\n",
    "y_train_imputed_reshaped_CNN = np.array(y_val_imputed.values).reshape((n_samples, 7))\n",
    "\n",
    "#8/8/20 idea: include goals as features\n",
    "X_train_with_goals = pd.concat([X_train_imputed, y_train_imputed], axis = 1)\n",
    "y_train_with_goals = pd.concat([X_val_imputed, y_val_imputed], axis = 1)\n",
    "\n",
    "X_train_imputed_reshaped_CNN = np.array(X_train_with_goals.values).reshape((n_samples, n_steps, n_features))\n",
    "#y_train_imputed_reshaped_CNN = np.array(y_train_with_goals.values).reshape((n_samples, n_features))\n",
    "\n",
    "#add a callback to stop training when no improvement in loss\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 15)\n",
    "\n",
    "#CNN model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(padding = 'same', filters = n_filters, kernel_size = kernel_size, activation = 'relu', \n",
    "                input_shape = (n_steps, n_features)))\n",
    "model.add(MaxPooling1D(pool_size = pool_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_1_output, activation = 'relu'))\n",
    "model.add(Dense(dense_2_output, activation = 'relu'))\n",
    "model.add(Dense(dense_3_output, activation = 'sigmoid'))\n",
    "model.add(Dense(dense_4_output))\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "history = model.fit(X_train_imputed_reshaped_CNN, y_train_imputed_reshaped_CNN, epochs = epochs, callbacks = [early_stop], verbose = False)\n",
    "\n",
    "#store results\n",
    "listhist = np.array(history.history['loss'])\n",
    "CNN_hyperparam_to_loss_dict[listhist[-1]] = h_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': ('filters', 'ksize', 'psize', 'epochs', 'd1out'),\n",
       " 0.00010645977522259596: [71, 4, 4, 120, 780],\n",
       " 0.0017688923056246102: [71, 4, 4, 20, 780],\n",
       " 0.0018683101091899466: [71, 4, 4, 20, 780],\n",
       " 0.005416893017278096: [71, 4, 4, 20, 780],\n",
       " 0.002077976874920113: [71, 4, 4, 20, 780],\n",
       " 0.0021870021869332713: [78, 4, 4, 20, 780],\n",
       " 0.0020212127901091475: [156, 4, 4, 20, 780],\n",
       " 0.0016693142797606431: [156, 6, 4, 20, 780],\n",
       " 0.0017621701476642879: [156, 7, 4, 20, 780],\n",
       " 0.0025034226155100023: [156, 7, 7, 20, 780],\n",
       " 0.001648060300726896: [156, 7, 3, 20, 780],\n",
       " 0.0021598441428374326: [78, 7, 3, 20, 780],\n",
       " 0.0020548045530775997: [234, 7, 3, 20, 780],\n",
       " 0.0014651812206100776: [156, 7, 3, 20, 780],\n",
       " 0.0014993841476982164: [156, 7, 3, 20, 780],\n",
       " 0.001950917174717603: [156, 7, 3, 20, 780],\n",
       " 0.0017856983380897023: [156, 7, 3, 20, 780],\n",
       " 0.0014858862306818227: [156, 7, 3, 20, 780],\n",
       " 0.00010724324578251777: [156, 7, 3, 120, 780],\n",
       " 6.795012470711213e-05: [78, 7, 3, 120, 780],\n",
       " 5.2411585716284796e-05: [78, 7, 3, 120, 780],\n",
       " 0.00011887657551299432: [78, 7, 3, 120, 780],\n",
       " 5.8873675349625436e-05: [78, 7, 3, 130, 780],\n",
       " 0.00012174887973249445: [78, 7, 3, 100, 780],\n",
       " 8.870828775721646e-05: [78, 7, 3, 110, 780],\n",
       " 4.607853036325982e-05: [78, 3, 3, 120, 780],\n",
       " 6.249023819147504e-05: [78, 5, 3, 120, 780],\n",
       " 0.00020404023969128172: [78, 78, 3, 120, 780],\n",
       " 5.6737273995150925e-05: [78, 7, 3, 120, 780],\n",
       " 7.252993709358788e-05: [78, 7, 3, 120, 780],\n",
       " 5.0904868581608614e-05: [78, 7, 3, 120, 780],\n",
       " 0.00022580074818226046: [78, 7, 3, 120, 78],\n",
       " 0.00014554035972812535: [78, 7, 3, 120, 156],\n",
       " 5.4423021988738674e-05: [78, 7, 3, 120, 780]}"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_hyperparam_to_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after 120 / 120 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xaa83b775c8>]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iters = len(history.history['loss'])\n",
    "x = [i for i in range(len(history.history['loss']))]\n",
    "print(\"Stopped after %i / %i iterations\" % (iters, epochs))\n",
    "\n",
    "window = iters - 10\n",
    "plt.plot(x[window:], listhist[window:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Test Set:  0.0008846143006179362\n",
      "Unimputed MSE for Test Set:  0.0007853896729013954\n",
      "\n",
      "MSE for Train Set:  5.855493902801191e-05\n",
      "Unimputed MSE for Train Set:  4.926222934487768e-05\n"
     ]
    }
   ],
   "source": [
    "#Create a validation set and predict for 2007\n",
    "\n",
    "#Get a training dataframe for years 1973-2005\n",
    "predict_indices = X_features_filled.loc['Afghanistan'].index[1:34].tolist()\n",
    "X_indicators = X_train_imputed.loc[pd.IndexSlice[:, predict_indices], :]\n",
    "X_targets = y_train_imputed.loc[pd.IndexSlice[:, predict_indices], :]\n",
    "X_val_CNN = pd.concat([X_indicators, X_targets], axis = 1)\n",
    "assert(X_val_CNN.isna().any().any() == False)\n",
    "\n",
    "#Get original validation set, badly named, and append goals to indicators\n",
    "X_2006 = pd.concat([X_val_imputed, y_val_imputed], axis = 1)\n",
    "assert(X_2006.isna().any().any() == False)\n",
    "\n",
    "#append 2006 to validation set\n",
    "X_val_CNN = pd.concat([X_val_CNN, X_2006], axis = 0)\n",
    "X_val_CNN = X_val_CNN.sort_index()\n",
    "assert(X_val_CNN.isna().any().any() == False)\n",
    "\n",
    "#reshape and make prediction\n",
    "x_val_CNN_reshaped = np.array(X_val_CNN.values).reshape(x_shape)\n",
    "y_hat = model.predict(x_val_CNN_reshaped)\n",
    "\n",
    "mse_test = mean_squared_error(y_hat, y_test_imputed)\n",
    "unim_cost, y_hat_filtered, y_test_filt = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'squared')\n",
    "\n",
    "print('MSE for Test Set: ', mse_test)\n",
    "print('Unimputed MSE for Test Set: ', unim_cost)\n",
    "\n",
    "#Compare to prediction on training data\n",
    "y_hat_train = model.predict(X_train_imputed_reshaped_CNN)\n",
    "mse_train = mean_squared_error(y_hat_train, y_val_imputed)\n",
    "\n",
    "unim_cost_train, y_hat_filtered_train, y_test_filt_train = compute_unimputed_cost(y_valid, y_hat_train, y_val_imputed, 'squared')\n",
    "\n",
    "print('\\nMSE for Train Set: ', mse_train)\n",
    "print('Unimputed MSE for Train Set: ', unim_cost_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "Baseline MSE for 2006 Target Predictions: 0.001493 \n",
    " Baseline unimputed MSE for 2006 Target Prediction 0.003208\n",
    "\n",
    "Baseline MSE for 2007 Target Predictions: 0.000919 \n",
    " Baseline unimputed MSE for 2007 Target Prediction 0.000735\n",
    "\n",
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B: Multiple Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Each feature (indicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "------\n",
    "----\n",
    "# Statistical Analysis From Early Stages \n",
    "\n",
    "These models were built to map indicators to goals without making use of the nature of time series data. They are not useless, though currently not being used. I have future plans. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                                    Step 5.1: MultiOutput Regression\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#Get a list of multioutput regressors\n",
    "mo_regressors = [(\"KNeighbors\", KNeighborsRegressor()), (\"Linear Regression\", LinearRegression()), (\"Random Forest\", RandomForestRegressor(n_estimators = 25))]\n",
    "prediction_dict = {}\n",
    "cost_dict = {}\n",
    "\n",
    "print(\"Starting MultiOutput Regression with multiple models . . .\")\n",
    "\n",
    "#Fit each model to imputed training data, then predict\n",
    "for model in mo_regressors:\n",
    "    model[1].fit(X_train_imputed, y_train_imputed)\n",
    "    print(\"Fit \" + model[0])\n",
    "    \n",
    "    prediction_dict[model[0]] = model[1].predict(X_test_imputed)\n",
    "    print(\"Predictions made with \" + model[0])\n",
    "    \n",
    "print(\"Calculating errors . . . \")\n",
    "#5 - calculate error for each model \n",
    "for prediction in prediction_dict:\n",
    "    \n",
    "    y_hat = prediction_dict[prediction]\n",
    "    \n",
    "    unimp_cost_sq, _, _ = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'squared')\n",
    "    unimp_cost_abs, _, _ = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'absolute')\n",
    "    imp_cost_sq = mean_squared_error(y_hat, y_test_imputed)\n",
    "    imp_cost_abs = mean_absolute_error(y_hat, y_test_imputed)\n",
    "    \n",
    "    cost_dict[prediction + \"_unimputed_cost_sq\"] = unimp_cost_sq\n",
    "    cost_dict[prediction + \"_unimputed_cost_abs\"] = unimp_cost_sq\n",
    "    cost_dict[prediction + \"_imputed_cost_sq\"] = imp_cost_sq\n",
    "    cost_dict[prediction + \"_imputed_cost_abs\"] = imp_cost_sq\n",
    "    \n",
    "    print(\"Error analysis for %s complete\" % prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print results for multioutput models\n",
    "print(\"Sorted in ascending order: \\n\")\n",
    "for cost in sorted(cost_dict, key=cost_dict.get, reverse=False):\n",
    "    print(cost, cost_dict[cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Forest_unimputed_cost_sq 0.0001655662543860639\n",
    "Random Forest_unimputed_cost_abs 0.0001655662543860639\n",
    "Random Forest_imputed_cost_sq 0.0003427480761347908\n",
    "Random Forest_imputed_cost_abs 0.0003427480761347908\n",
    "KNeighbors_unimputed_cost_sq 0.0005737431992566561\n",
    "KNeighbors_unimputed_cost_abs 0.0005737431992566561\n",
    "Linear Regression_unimputed_cost_sq 0.0008768644934505436\n",
    "Linear Regression_unimputed_cost_abs 0.0008768644934505436\n",
    "Linear Regression_imputed_cost_sq 0.001335331308048639\n",
    "Linear Regression_imputed_cost_abs 0.001335331308048639\n",
    "KNeighbors_imputed_cost_sq 0.001883435669439014\n",
    "KNeighbors_imputed_cost_abs 0.001883435669439014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                                Step 5.2: Wrapper MultiOutput Regression\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "#from sklearn.svm import LinearSVM\n",
    "\n",
    "#Get a list of multioutput regressors\n",
    "mo_regressors = [(\"KNeighbors\", KNeighborsRegressor()), (\"Linear Regression\", LinearRegression()), (\"Random Forest 5\", RandomForestRegressor(n_estimators = 5))]\n",
    "prediction_dict = {}\n",
    "cost_dict = {}\n",
    "\n",
    "print(\"Starting Wrapper MultiOutput Regression with multiple models . . .\")\n",
    "\n",
    "#Fit each model to imputed training data, then predict\n",
    "for model in mo_regressors:\n",
    "    wrapper = MultiOutputRegressor(model[1])\n",
    "    wrapper.fit(X_train_imputed, y_train_imputed)\n",
    "    print(\"Fit \" + model[0])\n",
    "    \n",
    "    prediction_dict[model[0]] = wrapper.predict(X_test_imputed)\n",
    "    print(\"Predictions made with \" + model[0])\n",
    "    \n",
    "print(\"Calculating errors . . . \")\n",
    "#5 - calculate error for each model \n",
    "for prediction in prediction_dict:\n",
    "    \n",
    "    y_hat = prediction_dict[prediction]\n",
    "    \n",
    "    unimp_cost_sq, _, _ = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'squared')\n",
    "    unimp_cost_abs, _, _ = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'absolute')\n",
    "    imp_cost_sq = mean_squared_error(y_hat, y_test_imputed)\n",
    "    imp_cost_abs = mean_absolute_error(y_hat, y_test_imputed)\n",
    "    \n",
    "    cost_dict[prediction + \"_unimputed_cost_sq\"] = unimp_cost_sq\n",
    "    cost_dict[prediction + \"_unimputed_cost_abs\"] = unimp_cost_sq\n",
    "    cost_dict[prediction + \"_imputed_cost_sq\"] = imp_cost_sq\n",
    "    cost_dict[prediction + \"_imputed_cost_abs\"] = imp_cost_sq\n",
    "    \n",
    "    print(\"Error analysis for %s complete\" % prediction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print results for multioutput models\n",
    "print(\"Sorted in ascending order: \\n\")\n",
    "for cost in sorted(cost_dict, key=cost_dict.get, reverse=False):\n",
    "    print(cost, cost_dict[cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest with wrapper seems to work the best so far, lets see the results\n",
    "\n",
    "#Get a list of multioutput regressors\n",
    "mo_regressors = [(\"5\", RandomForestRegressor(n_estimators = 5)), (\"40\", RandomForestRegressor(n_estimators = 20)), (\"50\", RandomForestRegressor(n_estimators = 30))]\n",
    "prediction_dict = {}\n",
    "cost_dict_wrapper_rf = {}\n",
    "\n",
    "print(\"Starting Wrapper MultiOutput Regression with multiple models . . .\")\n",
    "\n",
    "#Fit each model to imputed training data, then predict\n",
    "for model in mo_regressors:\n",
    "    wrapper = MultiOutputRegressor(model[1])\n",
    "    wrapper.fit(X_train_imputed, y_train_imputed)\n",
    "    print(\"Fit \" + model[0])\n",
    "    \n",
    "    prediction_dict[model[0]] = wrapper.predict(X_test_imputed)\n",
    "    print(\"Predictions made with \" + model[0])\n",
    "    \n",
    "print(\"Calculating errors . . . \")\n",
    "#5 - calculate error for each model \n",
    "for prediction in prediction_dict:\n",
    "    \n",
    "    y_hat = prediction_dict[prediction]\n",
    "    \n",
    "    unimp_cost_sq, _, _ = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'squared')\n",
    "    unimp_cost_abs, _, _ = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'absolute')\n",
    "    imp_cost_sq = mean_squared_error(y_hat, y_test_imputed)\n",
    "    imp_cost_abs = mean_absolute_error(y_hat, y_test_imputed)\n",
    "    \n",
    "    cost_dict_wrapper_rf[prediction + \"_unimputed_cost_sq\"] = unimp_cost_sq\n",
    "    cost_dict_wrapper_rf[prediction + \"_unimputed_cost_abs\"] = unimp_cost_sq\n",
    "    cost_dict_wrapper_rf[prediction + \"_imputed_cost_sq\"] = imp_cost_sq\n",
    "    cost_dict_wrapper_rf[prediction + \"_imputed_cost_abs\"] = imp_cost_sq\n",
    "    \n",
    "    print(\"Error analysis for %s complete\" % prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Costs for Wrapped RF chains in ascending order: \\n\")\n",
    "for cost in sorted(cost_dict_wrapper_rf, key=cost_dict_wrapper_rf.get, reverse=False):\n",
    "    print(cost, cost_dict_wrapper_rf[cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print results for multioutput models (only RFs)\n",
    "print(\"Sorted in ascending order: \\n\")\n",
    "for cost in sorted(cost_dict, key=cost_dict.get, reverse=False):\n",
    "    print(cost, cost_dict[cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                                    Step 5.1: MultiOutput Regression, RandForests\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#Get a list of multioutput regressors\n",
    "mo_regressors = [(\"RF - 5\", RandomForestRegressor(n_estimators = 5)), (\"RF - 10\", RandomForestRegressor(n_estimators = 10)), (\"RF - 25\", RandomForestRegressor(n_estimators = 25)), (\"RF - 50\", RandomForestRegressor(n_estimators = 50))]\n",
    "prediction_dict = {}\n",
    "cost_dict = {}\n",
    "\n",
    "print(\"Starting MultiOutput Regression with multiple models . . .\")\n",
    "\n",
    "#Fit each model to imputed training data, then predict\n",
    "for model in mo_regressors:\n",
    "    model[1].fit(X_train_imputed, y_train_imputed)\n",
    "    print(\"Fit \" + model[0])\n",
    "    \n",
    "    prediction_dict[model[0]] = model[1].predict(X_test_imputed)\n",
    "    print(\"Predictions made with \" + model[0])\n",
    "    \n",
    "print(\"Calculating errors . . . \")\n",
    "#5 - calculate error for each model \n",
    "for prediction in prediction_dict:\n",
    "    \n",
    "    y_hat = prediction_dict[prediction]\n",
    "    \n",
    "    unimp_cost_sq, _, _ = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'squared')\n",
    "    unimp_cost_abs, _, _ = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'absolute')\n",
    "    imp_cost_sq = mean_squared_error(y_hat, y_test_imputed)\n",
    "    imp_cost_abs = mean_absolute_error(y_hat, y_test_imputed)\n",
    "    \n",
    "    cost_dict[prediction + \"_unimputed_cost_sq\"] = unimp_cost_sq\n",
    "    cost_dict[prediction + \"_unimputed_cost_abs\"] = unimp_cost_sq\n",
    "    cost_dict[prediction + \"_imputed_cost_sq\"] = imp_cost_sq\n",
    "    cost_dict[prediction + \"_imputed_cost_abs\"] = imp_cost_sq\n",
    "    \n",
    "    print(\"Error analysis for %s complete\" % prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sorted in ascending order: \\n\")\n",
    "for cost in sorted(cost_dict, key=cost_dict.get, reverse=False):\n",
    "    print(cost, cost_dict[cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "unimp_cost_sq, y_hat_filt, y_test_filt = compute_unimputed_cost(y_test, y_hat_knr, y_test_imputed, 'squared')\n",
    "unimp_cost_abs, y_hat_filt, y_test_filt = compute_unimputed_cost(y_test, y_hat_knr, y_test_imputed, 'absolute')\n",
    "\n",
    "imp_cost_sq = mean_squared_error(y_hat_knr, y_test_imputed)\n",
    "imp_cost_abs = mean_absolute_error(y_hat_knr, y_test_imputed)\n",
    "\n",
    "print('Imputed_abs: %f\\nImputed_sq: %f\\nUnimputed_abs: %f \\nUnimputed_sq: %f' % (imp_cost_abs, imp_cost_sq, unimp_cost_abs, unimp_cost_sq))\n",
    "\n",
    "y_hat_knr_train = my_model.predict(X_train_imputed)\n",
    "\n",
    "train_unimp_cost_sq, y_hat_filt, y_test_filt = compute_unimputed_cost(y_train, y_hat_knr_train, y_train_imputed, 'squared')\n",
    "train_unimp_cost_abs, y_hat_filt, y_test_filt = compute_unimputed_cost(y_train, y_hat_knr_train, y_train_imputed, 'absolute')\n",
    "\n",
    "train_imp_cost_sq = mean_squared_error(y_hat_knr_train, y_train_imputed)\n",
    "train_imp_cost_abs = mean_absolute_error(y_hat_knr_train, y_train_imputed)\n",
    "\n",
    "print('Training:')\n",
    "print('Imputed_abs: %f\\nImputed_sq: %f\\nUnimputed_abs: %f \\nUnimputed_sq: %f' % (train_imp_cost_abs, train_imp_cost_sq, train_unimp_cost_abs, train_unimp_cost_sq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------------------------------------\n",
    "\n",
    "    Step 5: Create model, fit, and predict \n",
    "    \n",
    "                    CROSS VALIDATION\n",
    "\n",
    "--------------------------------------------------------------'''\n",
    "\n",
    "from numpy import absolute\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "targets_y = targets_y.fillna(targets_y.mean())\n",
    "features_X = features_X.fillna(features_X.mean())\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=5)\n",
    "wrapper = MultiOutputRegressor(RandomForestRegressor(n_estimators = 5))\n",
    "scoring = 'r2'\n",
    "results = model_selection.cross_val_score(wrapper, features_X, targets_y, cv=kfold, scoring=scoring)\n",
    "\n",
    "#print(\"R^2: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapley Values : Determine which indicators of all of them have the greatest impact on determingin the targets, then build a model based\n",
    "#    on thse specific indicators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Idea: run seperate models for each target. As for projecting into the future, find a way to accurately interpolate a countries indicator values in the years 2008 and 2012, after deciding which are the best indicators to use (both shapley and threshold) and then create an indicator vector accordingly and predict its target value accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest with wrapper seems to work the best so far, lets see the results\n",
    "\n",
    "#Get a list of multioutput regressors\n",
    "mo_regressors = [(\"5.1\", RandomForestRegressor(n_estimators = 5)), (\"5.2\", RandomForestRegressor(n_estimators = 5)), (\"5.3\", RandomForestRegressor(n_estimators = 5))]\n",
    "prediction_dict = {}\n",
    "cost_dict_wrapper_rf = {}\n",
    "\n",
    "print(\"Starting Wrapper MultiOutput Regression with multiple models . . .\")\n",
    "\n",
    "#Fit each model to imputed training data, then predict\n",
    "for model in mo_regressors:\n",
    "    wrapper = MultiOutputRegressor(model[1])\n",
    "    wrapper.fit(X_train_imputed, y_train_imputed)\n",
    "    print(\"Fit \" + model[0])\n",
    "    \n",
    "    prediction_dict[model[0]] = wrapper.predict(X_test_imputed)\n",
    "    print(\"Predictions made with \" + model[0])\n",
    "    \n",
    "print(\"Calculating errors . . . \")\n",
    "#5 - calculate error for each model \n",
    "for prediction in prediction_dict:\n",
    "    \n",
    "    y_hat = prediction_dict[prediction]\n",
    "    \n",
    "    unimp_cost_sq, _, _ = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'squared')\n",
    "    unimp_cost_abs, _, _ = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'absolute')\n",
    "    imp_cost_sq = mean_squared_error(y_hat, y_test_imputed)\n",
    "    imp_cost_abs = mean_absolute_error(y_hat, y_test_imputed)\n",
    "    \n",
    "    cost_dict_wrapper_rf[prediction + \"_unimputed_cost_sq\"] = unimp_cost_sq\n",
    "    cost_dict_wrapper_rf[prediction + \"_unimputed_cost_abs\"] = unimp_cost_sq\n",
    "    cost_dict_wrapper_rf[prediction + \"_imputed_cost_sq\"] = imp_cost_sq\n",
    "    cost_dict_wrapper_rf[prediction + \"_imputed_cost_abs\"] = imp_cost_sq\n",
    "    \n",
    "    print(\"Error analysis for %s complete\" % prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Costs for Wrapped RF chains in ascending order: \\n\")\n",
    "for cost in sorted(cost_dict_wrapper_rf, key=cost_dict_wrapper_rf.get, reverse=False):\n",
    "    print(cost, cost_dict_wrapper_rf[cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                                    Step 5.3: Fully Connected Neural Network (Keras)\n",
    "                                    \n",
    "                              *******UNDER CONSTRUcTION -- COME BACK LATER******\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "input_dim = 468\n",
    "output_dim = 7\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(230, input_dim = input_dim, activation = 'relu'))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(Dense(output_dim))\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_imputed.to_numpy(), y_train_imputed.to_numpy(), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                        FEATURE IMPORTANCE -- Attempt using Random Forest with Wrapper \n",
    "                                    \n",
    "-------------------------------------------------------------------------------------------------------------------------'''\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 5)\n",
    "wrapper = MultiOutputRegressor(model)\n",
    "\n",
    "wrapper.fit(X_train_imputed, y_train_imputed)\n",
    "y_hat = wrapper.predict(X_test_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "unimp_cost_sq, x, y = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'squared')\n",
    "unimp_cost_abs, x1, y1 = compute_unimputed_cost(y_test, y_hat, y_test_imputed, 'absolute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unimp_cost_sq, unimp_cost_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "model.fit(X_train_imputed, y_train_imputed.iloc[:,4])\n",
    "perm = PermutationImportance(model, random_state = 1).fit(X_train_imputed, y_train_imputed.iloc[:,4])\n",
    "eli5.show_weights(perm, feature_names = X_train_imputed.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: create model using all of the indicators, use shap and perm importance to pick the most importanr\n",
    "    also try using lasso and ridge for regularization and see which parameters they pick\n",
    "    also use multivariate forecasting to determine values of inidcators in 2008 and 2012. As an experiment, leave the targets in and compare the predictions with \n",
    "    the values from the models predictions\n",
    "    lastly for imputation, group countries by region and/or by socioeconomic sphere, then seperate, impute, and append\n",
    "    \n",
    "    Later model, break down each target by the indicators that specifically correspond to it(check uN website), create seperate \n",
    "    indicator dataframes for each target, and impute all of those accordingly\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to improve on this project:\n",
    "    \n",
    "    1. Multivariate Time Series Imputation         \n",
    "        a. https://www.groundai.com/project/multivariate-time-series-imputation-with-variational-autoencoders/1\n",
    "        b. a. https://papers.nips.cc/paper/7432-multivariate-time-series-imputation-with-generative-adversarial-networks.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
